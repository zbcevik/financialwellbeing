---
title: 'CAPSTONE PROJECT'
output:
  html_document: default
  word_document: default
  pdf_document: default
---
<center> <h1> Initial Results </h1> </center>
<center>  <h3> [Zeynep Burcu Cevik] </h2> </center>
<center> <h3> [ 501138112] </h2> </center>
---

#### 1. Importing dataset and choosing the already decided attributes
```{r}


fwb<-read.csv("/Users/zeynepcevik/Desktop/Ryerson/CIND820_Fall/Financial_wellbeing/NFWBS_PUF_2016_data.csv", header=TRUE, sep = ",", stringsAsFactors = TRUE)

subfwb<-data.frame(fpl=fwb$fpl, FWBscore=fwb$FWBscore, FSscore=fwb$FSscore, LMscore=fwb$LMscore, KHscore=fwb$KHscore, EMPLOY=fwb$EMPLOY, Military_Status=fwb$Military_Status, agecat=fwb$agecat, PPEDUC=fwb$PPEDUC, PPETHM=fwb$PPETHM, PPGENDER=fwb$PPGENDER, PPHHSIZE=fwb$PPHHSIZE,PPINCIMP= fwb$PPINCIMP, PPMARIT=fwb$PPMARIT, PPMSACAT=fwb$PPMSACAT, PPREG4=fwb$PPREG4, PPT01=fwb$PPT01, PPT25=fwb$PPT25, PPT612=fwb$PPT612, PPT1317=fwb$PPT1317, PPT18OV=fwb$PPT18OV)
library(dplyr)
subfwb<-na.omit(subfwb)
#Eliminate the participants who refused to respond. 
#From the data description, among these variables, only in employment and military status attributes, there are non-respondents.

subfwb<-filter(subfwb, subfwb$EMPLOY !=99 & subfwb$Military_Status !=-1 & subfwb$FWBscore >0)


```

#### 2. Divide the dataset to training and test sets.
```{r}

set.seed(1234)
train_index <-sample(1:nrow(subfwb), 0.8 * nrow(subfwb))
train.set <- subfwb[train_index,]
test.set <- subfwb[-train_index,]

```

#### 3. Build the model (Decision Tree)
```{r}
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
fit<-rpart(FWBscore~., data=train.set)
rpart.plot(fit, extra=101)
rpart.rules (fit, roundint = FALSE)

```

#### 4. Predict FWB Score
```{r}

predicted<-predict(fit, test.set)
#ConfusionMatrix<-table(actual = test.set$FWBscore,predicted = predicted)
#print(ConfusionMatrix)
#ConfusionMatrix_discrete<-confusionMatrix(as.factor(predicted), as.factor(test.set$FWBscore))
head(predicted)
#print(ConfusionMatrix_discrete)
plot(test.set$FWBscore, predicted, xlab="y", ylab=expression(hat(y)),pch=20, xlim=c(10,100), ylim=c(10,100))
plot(abs(test.set$FWBscore-predicted),ylab="Residuals (Actual-Predicted)", xlab="Observation")
```

#### 5. Turn FWB Score into categorical variable
```{r}

# FWBcat <- 0-25: very low score-1 , 26-50: low score-2, 51-75: high score - 3, 76-100: very high score -4
# FWBcat_bin <- 0-50 : low score - 1, 51-100: high score -2
subfwbcat<- subfwb[ , c(1,3:21)]
subfwbcat_bin<-subfwb[ , c(1,3:21)]
for(i in 1:nrow(subfwb)){
  if (subfwb$FWBscore[i] <=25){
    subfwbcat$FWBcat[i] <-as.integer(1)
    subfwbcat_bin$FWBcat_bin[i]<-as.integer(1)
  }
  else if (subfwb$FWBscore[i] <=50){
    subfwbcat$FWBcat[i] <-as.integer(2)
    subfwbcat_bin$FWBcat_bin[i]<-as.integer(1)
  }
   else if (subfwb$FWBscore[i] <=75){
    subfwbcat$FWBcat[i] <-as.integer(3)
    subfwbcat_bin$FWBcat_bin[i]<-as.integer(2)
   }
  else if (subfwb$FWBscore[i] <=100){
    subfwbcat$FWBcat[i] <-as.integer(4)
    subfwbcat_bin$FWBcat_bin[i]<-as.integer(2)
  }
}

#class(subfwbcat$FWBcat)
#class(subfwbcat_bin$FWBcat_bin)


#as.factor(subfwbcat_bin$FWBcat_bin)

subfwbcat[,'FWBcat'] <- as.factor(subfwbcat[,'FWBcat'])
subfwbcat_bin[,'FWBcat_bin'] <- as.factor(subfwbcat_bin[,'FWBcat_bin'])


str(subfwbcat)

str(subfwbcat_bin)
```

#### 6. Divide the dataset to training and test sets.
```{r}

set.seed(1234)
traincat_index <-sample(1:nrow(subfwbcat), 0.8 * nrow(subfwbcat))
traincat.set <- subfwbcat[traincat_index,]
testcat.set <- subfwbcat[-traincat_index,]

set.seed(123)
traincatbin_index <-sample(1:nrow(subfwbcat_bin), 0.8 * nrow(subfwbcat_bin))
traincat_bin.set <- subfwbcat_bin[traincatbin_index,]
testcat_bin.set <- subfwbcat_bin[-traincatbin_index,]

```
#### 7. Build two models (Decision Tree)
```{r}
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
fitcat<-rpart(FWBcat~., data=traincat.set, method="class")
fitcat_bin<-rpart(FWBcat_bin~., data=traincat_bin.set, method="class")
rpart.plot(fitcat, extra=101)
rpart.rules (fitcat, roundint = FALSE)
rpart.plot(fitcat_bin, extra=101)
rpart.rules (fitcat_bin, roundint = FALSE)
```

#### 8. Confusion Matrix  
```{r}
#install.packages("caret")
#install.packages("ggplot2")
library(caret)
decisiontree_predicted_catbin<-predict(fitcat_bin, type="class")
ConfusionMatrix_decisiontree<-confusionMatrix(decisiontree_predicted_catbin, traincat_bin.set$FWBcat_bin)
print(ConfusionMatrix_decisiontree)

```


#### 9. Logistic Regression for two datasets 
```{r}

glm_model1<- glm(FWBcat~., data=traincat.set, family="binomial")
summary(glm_model1)

glm_model2<- glm(FWBcat_bin~., data=traincat_bin.set, family="binomial")
summary(glm_model2)
null_glm<-glm(FWBcat_bin~1, data=traincat_bin.set, family="binomial")
1-logLik(glm_model2)/logLik(null_glm)

glm_model3<- glm(FWBcat_bin~FSscore+KHscore+agecat+PPETHM+PPINCIMP+PPMARIT, data=traincat_bin.set, family="binomial")
summary(glm_model3)
1-logLik(glm_model3)/logLik(null_glm)

```


#### 10.Predict FWB Categorical Variable for two datasets
```{r}
library(caret)

predictedcat<-predict(glm_model1, testcat.set, type="response" )
predicted_class1 <- ifelse(predictedcat<=0.25, 1, ifelse(predictedcat<=0.50,2, ifelse(predictedcat<=0.75,3,ifelse(predictedcat<=1, 4))))
ConfusionMatrix1 <- table(actual = testcat.set$FWBcat ,predicted = predicted_class1)
print(ConfusionMatrix1)

ConfMa1<-confusionMatrix(as.factor(predicted_class1),testcat.set$FWBcat)
print(ConfMa1)

predictedcat_bin<-predict(glm_model2, testcat_bin.set, type="response" )
predicted_class2 <- ifelse(predictedcat_bin>=0.5, "2", "1")
ConfusionMatrix2 <- table(actual = testcat_bin.set$FWBcat_bin ,predicted = predicted_class2)
print(ConfusionMatrix2)

ConfMa2<-confusionMatrix(as.factor(predicted_class2),testcat_bin.set$FWBcat_bin )
print(ConfMa2)


```
#### 11. Build two models (Decision Tree)
```{r}
library("party")
r2 <- ctree(glm_model2, data=traincat_bin.set)
plot(r2)

```
#### 12. Apply Ordinary Least Square and apply forward selection and backward elimination algorithms to the dataset
```{r}
install.packages("FNN")
install.packages("RCurl")
install.packages("MASS")
install.packages("leaps")
library(MASS) # stepwise regression
library(leaps) # all subsets regression
#Forward Selection
full <- lm(FWBscore~.,data=subfwb)
null <- lm(FWBscore~1,data=subfwb)
stepF <- stepAIC(null, scope=list(lower=null, upper=full),direction= "forward", trace=TRUE)
summary(stepF)



```

#### 14. Residuals of OLS model
```{r}
fit_ols=lm(formula = FWBscore ~ FSscore + agecat + PPINCIMP + KHscore + EMPLOY + PPHHSIZE + PPREG4 + PPMARIT + PPETHM + PPT1317 + Military_Status, data = train.set)
plot(fit_ols$residuals, ylab="Residuals")
```
#### 13. Apply Elbow MEthod to find an optimal k-value for k-NN model.
```{r}
#install.packages("sjPlot")
#library(sjPlot)
#sjc.elbow(traincat_bin.set)
library(FNN) 
library(class)
library(gmodels)
library(caret)
error_rate<-c()
m<-c()
# Will take some time

for (i in 1:40) {
 prc_test_predmm <-knn.reg(train = train.set[,-2], test=test.set[,-2], y = train.set[,2], k=i)
 ss <- 0
  for (j in 1:1248) {
     ss <- ss + (test.set$FWBscore[j] - prc_test_predmm$pred[j])^2
  }
  error_rate[i] <- sqrt(ss)
  m[i]<-i
}
plot(error_rate, ylab="Sum of Squared Residuals",xlab="K value in k-NN", pch=20)
```


#### 13. k-NN classification with confusion matrix (for categorical FWB variable-two datasets) + k-NN regression with error plot (FWB numerical variable)

```{r}
#install.packages("class")
#install.packages("gmodels")
library(FNN) 
library(class)
library(gmodels)
library(caret)

prc_test_pred <- knn(train = traincat.set[,1:20], test=testcat.set[,1:20], cl = traincat.set[,21], k=15)
table(Actual=testcat.set$FWBcat, Predicted=prc_test_pred)
ConfMa1<-confusionMatrix(as.factor(prc_test_pred),testcat.set$FWBcat)
print(ConfMa1)

prc_test_pred2 <- knn(train = traincat_bin.set[,1:20], test=testcat_bin.set[,1:20], cl = traincat_bin.set[,21], k=15)
table(Actual=testcat_bin.set$FWBcat, Predicted=prc_test_pred2)

ConfMa2<-confusionMatrix(as.factor(prc_test_pred2),testcat_bin.set$FWBcat_bin)
print(ConfMa2)

prc_test_pred3 <- knn.reg(train = train.set[,-2], test=test.set[,-2], y = train.set[,2], k=15)

#Plot the predicted results printed above against the actual values of the outcome variable test set.
plot(test.set$FWBscore, prc_test_pred3$pred, xlab="y", ylab=expression(hat(y)))
#If the values were perfectly predicted, we would expect to see points along the y = x line (the lower-left to upper-right diagonal if the scales on each axis of the plot are the same). 


#mean square prediction error
mean((test.set$FWBscore - prc_test_pred3$pred) ^ 2)

#mean absolute error
mean(abs(test.set$FWBscore - prc_test_pred3$pred))

#better to interpret these numbers in comparison of the models.


```
#### 14. Conduct Naive Bayes Method
```{r}
#install.packages("e1071")
#install.packages("caTools")
#install.packages("caret")

library(e1071)
library(caTools)
library(caret)

# Fitting Naive Bayes Model to training dataset
classifier_cl<- naiveBayes(FWBcat_bin ~ ., data = traincat_bin.set)
classifier_cl


# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = testcat_bin.set)
 
# Confusion Matrix
cm <- table(testcat_bin.set$FWBcat_bin, y_pred)
cm
 
# Model Evaluation
confusionMatrix(cm)

```

#### 15. Plot the trained and test sets.  
```{r}
#install.packages("ggplot2")
library(ggplot2)
library(reshape)


plot(train.set$FWBscore,train.set$FSscore, xlim=c(0,100),pch=20, xlab="FWB Score", ylab="FS Score")
points(test.set$FWBscore,test.set$FSscore, col="red", pch=20)


```


